{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<logfire._internal.main.Logfire at 0x10aba31a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logfire\n",
    "logfire.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Logfire</span> project URL: <a href=\"https://logfire-us.pydantic.dev/gouarin/softwareteam\" target=\"_blank\"><span style=\"color: #008080; text-decoration-color: #008080; text-decoration: underline\">https://logfire-us.pydantic.dev/gouarin/softwareteam</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mLogfire\u001b[0m project URL: \u001b]8;id=587272;https://logfire-us.pydantic.dev/gouarin/softwareteam\u001b\\\u001b[4;36mhttps://logfire-us.pydantic.dev/gouarin/softwareteam\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from AISoftTeam.agents.analyst import Analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "#this is needed to run async functions sync in jupyter\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Analyst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = agent.invoke(\"Write a project using langGraph and pydantic_ai with Ollama models which simulates a developer team composed by an analyst, a coder, a tester and a reviewer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Alright, I need to figure out how to split the user's request into manageable steps for a junior developer. The user wants a Python project using LangGraph and pydantic-ai with Ollama models to simulate a developer team with roles: analyst, coder, tester, and reviewer.\n",
       "\n",
       "First, I'll start by understanding each component involved. LangGraph is a framework for building language models, and pydantic-ai adds validation and parsing capabilities. Ollama is another model, so integration will be key.\n",
       "\n",
       "I should break down the project into logical steps. The first step would be setting up the environment because without the right tools installed, nothing else can proceed. This includes installing Python, necessary libraries, and initializing a virtual environment to keep things organized.\n",
       "\n",
       "Next, I need to outline the overall architecture of the system. Deciding how each role interacts with the models is crucial. Each team member should have specific responsibilities, so defining classes or modules for each makes sense. Clear communication between these components will ensure everything works smoothly.\n",
       "\n",
       "Then, implementing each role as separate modules would allow for modularity and easier testing. For example, the analyst can handle requirements gathering, the coder writes the code, the tester checks it, and the reviewer provides feedback. Each module should interact with LangGraph and Ollama appropriately.\n",
       "\n",
       "After that, integrating all these modules into a main workflow is necessary. This step ensures that each role's output feeds into the next, creating a cohesive process. Handling exceptions and errors here will make the system robust.\n",
       "\n",
       "Finally, testing and refining each part ensures everything works as intended. Testing each module individually first, then the integrated system, helps catch issues early. Refinement based on feedback will polish the project.\n",
       "\n",
       "I should also provide detailed descriptions for each step to guide the developers clearly, ensuring they have all the information needed without extra research. Including web searches where necessary will help them find resources quickly.\n",
       "</think>\n",
       "\n",
       "```xml\n",
       "<root>\n",
       "    <request>\n",
       "        Develop a Python project using LangGraph and pydantic-ai with Ollama models that simulates a developer team consisting of an analyst, coder, tester, and reviewer.\n",
       "    </request>\n",
       "    \n",
       "    <step id=\"1\">\n",
       "        <full_description>\n",
       "            Set up the development environment and install all necessary dependencies for the project. This includes installing Python, setting up a virtual environment, and installing libraries such as LangGraph, pydantic-ai, and Ollama. The goal is to create a clean and manageable environment where all required tools are easily accessible.\n",
       "        </full_description>\n",
       "        <web_search>\n",
       "            - \"install python on ubuntu\"\n",
       "            - \"how to set up virtual environment in Python\"\n",
       "            - \"install langgraph using pip\"\n",
       "            - \"pydantic-ai installation guide\"\n",
       "            - \"ollama model installation for Python\"\n",
       "        </web_search>\n",
       "    </step>\n",
       "\n",
       "    <step id=\"2\">\n",
       "        <full_description>\n",
       "            Understand and implement the LangGraph framework to create a graph-based language model. This involves setting up nodes and edges that represent the relationships between different components of the developer team simulation. The graph will be used to model the interactions between the analyst, coder, tester, and reviewer.\n",
       "        </full_description>\n",
       "        <web_search>\n",
       "            - \"LangGraph documentation\"\n",
       "            - \"graph-based language models explained\"\n",
       "            - \"nodes and edges in LangGraph\"\n",
       "            - \"how to create a graph using LangGraph in Python\"\n",
       "        </web_search>\n",
       "    </step>\n",
       "\n",
       "    <step id=\"3\">\n",
       "        <full_description>\n",
       "            Implement the pydantic-ai library to handle data validation and parsing within the developer team simulation. This step involves creating schemas for each role (analyst, coder, tester, reviewer) and ensuring that all inputs and outputs conform to these schemas. The goal is to maintain data consistency and integrity throughout the simulation.\n",
       "        </full_description>\n",
       "        <web_search>\n",
       "            - \"pydantic-ai tutorial\"\n",
       "            - \"data validation with pydantic\"\n",
       "            - \"how to create schemas in pydantic\"\n",
       "            - \"parsing data with pydantic-ai\"\n",
       "        </web_search>\n",
       "    </step>\n",
       "\n",
       "    <step id=\"4\">\n",
       "        <full_description>\n",
       "            Integrate the Ollama model into the project. This involves setting up the Ollama server, configuring it to work with the LangGraph framework, and ensuring that the model can be accessed and used by each component of the developer team simulation. The integration should allow for seamless communication between the different roles.\n",
       "        </full_description>\n",
       "        <web_search>\n",
       "            - \"Ollama model installation\"\n",
       "            - \"how to integrate Ollama with Python\"\n",
       "            - \"Ollama server setup guide\"\n",
       "            - \"communication between LangGraph and Ollama\"\n",
       "        </web_search>\n",
       "    </step>\n",
       "\n",
       "    <step id=\"5\">\n",
       "        <full_description>\n",
       "            Design and implement the developer team simulation. This involves creating classes or modules for each role (analyst, coder, tester, reviewer) and defining their responsibilities and interactions within the system. Each role should be able to communicate with the others through the LangGraph framework and use the Ollama model to perform their tasks.\n",
       "        </full_description>\n",
       "        <web_search>\n",
       "            - \"object-oriented design in Python\"\n",
       "            - \"class modules for developer roles\"\n",
       "            - \"interactions between classes in a simulation\"\n",
       "            - \"how to implement team roles in Python\"\n",
       "        </web_search>\n",
       "    </step>\n",
       "\n",
       "    <step id=\"6\">\n",
       "        <full_description>\n",
       "            Test the integration of all components. This involves running simulations where each role performs their tasks, and ensuring that the interactions are smooth and produce the expected results. Testing should cover various scenarios to ensure robustness and reliability.\n",
       "        </full_description>\n",
       "        <web_search>\n",
       "            - \"unit testing in Python\"\n",
       "            - \"integration testing for complex systems\"\n",
       "            - \"how to test simulations in Python\"\n",
       "            - \"debugging tips for simulation code\"\n",
       "        </web_search>\n",
       "    </step>\n",
       "\n",
       "    <step id=\"7\">\n",
       "        <full_description>\n",
       "            Refine and optimize the system based on testing results. This involves identifying any bottlenecks or issues that arose during testing and implementing fixes or improvements. The goal is to create a seamless and efficient developer team simulation that accurately models real-world interactions.\n",
       "        </full_description>\n",
       "        <web_search>\n",
       "            - \"optimization techniques in Python\"\n",
       "            - \"how to improve simulation performance\"\n",
       "            - \"debugging complex systems\"\n",
       "            - \"best practices for code optimization\"\n",
       "        </web_search>\n",
       "    </step>\n",
       "</root>\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke(\"Create a function that reads a png file and return a NumPy array with YUV format using PIL package and nothing else.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Alright, I need to figure out how to create a Python function that reads a PNG file and returns a NumPy array in YUV format using only the PIL package. Let me break this down step by step.\n",
       "\n",
       "First, I know that PIL can open images, so I'll start by reading the image into a PIL Image object. That's straightforward with `Image.open()`. But wait, does PIL support all formats? Well, PNG is supported, so no issues there.\n",
       "\n",
       "Next, I need to convert this RGB image to YUV. I remember that YUV has different color spaces like YCbCr, which might be what PIL uses. So, I should look into how PIL handles color space conversions. Maybe the `convert()` method can help here. Oh right, using `.convert(\"YCbCr\")` will give me the YUV channels.\n",
       "\n",
       "Now, each channel in YCbCr is a separate layer. I need to extract these layers. I think splitting the image into its bands would work. Using `split()` on the converted image should give me three separate images: Y, Cb, and Cr.\n",
       "\n",
       "But wait, each of these is an Image object. To use them with NumPy, I need to convert them into arrays. So, using `np.array()` on each band will do that. However, I remember that PIL's array might have a different data type, so checking the dtype and normalizing if necessary is important.\n",
       "\n",
       "Once I have the Y, Cb, and Cr arrays, I need to combine them back into a single NumPy array in the correct order. Since YUV typically has the Y channel first, followed by U and V (or Cb and Cr), I'll stack them using `np.dstack()`.\n",
       "\n",
       "But wait, sometimes images are stored as height x width x channels, but I think that's already handled when stacking. Also, I should ensure that each channel is properly scaled. Maybe I need to normalize the values or apply any necessary transformations between PIL's output and standard YUV ranges.\n",
       "\n",
       "Putting it all together: open the image, convert to YCbCr, split into bands, convert each to a NumPy array, stack them, and return as YUV. I should also test this function with sample images to make sure it works correctly and handles any potential errors, like non-PNG files or unsupported formats.\n",
       "\n",
       "I might have missed something about the color space conversion, so double-checking PIL's documentation on `convert()` is essential. Also, ensuring that the output array has the correct dimensions and data types will be crucial for compatibility with other libraries expecting YUV input.\n",
       "</think>\n",
       "\n",
       "```xml\n",
       "<root>\n",
       "    <request>\n",
       "        Create a function in Python that reads a PNG file and returns a NumPy array in YUV format using only the PIL package.\n",
       "    </request>\n",
       "    \n",
       "    <step_1>\n",
       "        <full_description>\n",
       "            Read the PNG file into a PIL Image object. This involves using the PIL library to open the image file and ensuring it is properly loaded into memory. The function will take the file path as input and return an Image object.\n",
       "        </full_description>\n",
       "        <web_search>\n",
       "            - \"PIL open image from file\"\n",
       "            - \"Python PIL read PNG file\"\n",
       "            - \"Image.open() method documentation\"\n",
       "        </web_search>\n",
       "    </step_1>\n",
       "    \n",
       "    <step_2>\n",
       "        <full_description>\n",
       "            Convert the RGB image to YUV color space. This involves using the convert() method of the Image object to transform the image from its original color space (likely RGB) to YCbCr, which is a form of YUV.\n",
       "        </full_description>\n",
       "        <web_search>\n",
       "            - \"PIL Image convert to YCbCr\"\n",
       "            - \"RGB to YUV conversion using PIL\"\n",
       "            - \"YCbCr color space in PIL\"\n",
       "        </web_search>\n",
       "    </step_2>\n",
       "    \n",
       "    <step_3>\n",
       "        <full_description>\n",
       "            Split the YCbCr image into its individual channels. Each channel (Y, Cb, Cr) will be separated into their own Image objects using the split() method.\n",
       "        </full_description>\n",
       "        <web_search>\n",
       "            - \"Splitting image bands in PIL\"\n",
       "            - \"Image.split() method documentation\"\n",
       "            - \"Separating YCbCr channels with PIL\"\n",
       "        </web_search>\n",
       "    </step_3>\n",
       "    \n",
       "    <step_4>\n",
       "        <full_description>\n",
       "            Convert each channel to a NumPy array. This involves taking each individual Image object (Y, Cb, Cr) and converting them into NumPy arrays for further processing.\n",
       "        </full_description>\n",
       "        <web_search>\n",
       "            - \"Convert PIL Image to NumPy array\"\n",
       "            - \"NumPy array from PIL image\"\n",
       "            - \"PIL to NumPy conversion examples\"\n",
       "        </web_search>\n",
       "    </step_4>\n",
       "    \n",
       "    <step_5>\n",
       "        <full_description>\n",
       "            Combine the Y, Cb, and Cr arrays into a single NumPy array in the correct order. This involves stacking the individual channel arrays along the third dimension to form a YUV color space array.\n",
       "        </full_description>\n",
       "        <web_search>\n",
       "            - \"Stacking NumPy arrays for image processing\"\n",
       "            - \"NumPy dstack() function documentation\"\n",
       "            - \"Combining image channels in NumPy\"\n",
       "        </web_search>\n",
       "    </step_5>\n",
       "</root>\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_code_blocks(markdown_text):\n",
    "    \"\"\"\n",
    "    Extract code blocks from markdown text.\n",
    "    Returns a list of tuples (language, code_content)\n",
    "    \"\"\"\n",
    "    # Pattern to match code blocks with language specification\n",
    "    pattern = r\"```(\\w+)\\n(.*?)```\"\n",
    "    # Find all matches with re.DOTALL to match across multiple lines\n",
    "    matches = re.findall(pattern, markdown_text, re.DOTALL)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_blocks = extract_code_blocks(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "root = ET.fromstring(code_blocks[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'full_description' at 0x11fcc5940>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag: root, Attributes: {}\n",
      "Tag: request, Attributes: {}\n",
      "Tag: step, Attributes: {'id': '1'}\n",
      "Tag: full_description, Attributes: {}\n",
      "Tag: web_search, Attributes: {}\n",
      "Tag: step, Attributes: {'id': '2'}\n",
      "Tag: full_description, Attributes: {}\n",
      "Tag: web_search, Attributes: {}\n",
      "Tag: step, Attributes: {'id': '3'}\n",
      "Tag: full_description, Attributes: {}\n",
      "Tag: web_search, Attributes: {}\n",
      "Tag: step, Attributes: {'id': '4'}\n",
      "Tag: full_description, Attributes: {}\n",
      "Tag: web_search, Attributes: {}\n",
      "Tag: step, Attributes: {'id': '5'}\n",
      "Tag: full_description, Attributes: {}\n",
      "Tag: web_search, Attributes: {}\n",
      "Tag: step, Attributes: {'id': '6'}\n",
      "Tag: full_description, Attributes: {}\n",
      "Tag: web_search, Attributes: {}\n",
      "Tag: step, Attributes: {'id': '7'}\n",
      "Tag: full_description, Attributes: {}\n",
      "Tag: web_search, Attributes: {}\n"
     ]
    }
   ],
   "source": [
    "steps = []\n",
    "for elem in root.iter():\n",
    "    print(f\"Tag: {elem.tag}, Attributes: {elem.attrib}\")\n",
    "    if elem.tag == \"step\":\n",
    "        steps.append(Step(description=elem[0].text, web_search=elem[1].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Set up the development environment and install all necessary dependencies for the project. This includes installing Python, setting up a virtual environment, and installing libraries such as LangGraph, pydantic-ai, and Ollama. The goal is to create a clean and manageable environment where all required tools are easily accessible.\n",
      "        \n",
      "\n",
      "            - \"install python on ubuntu\"\n",
      "            - \"how to set up virtual environment in Python\"\n",
      "            - \"install langgraph using pip\"\n",
      "            - \"pydantic-ai installation guide\"\n",
      "            - \"ollama model installation for Python\"\n",
      "        \n",
      "\n",
      "            Understand and implement the LangGraph framework to create a graph-based language model. This involves setting up nodes and edges that represent the relationships between different components of the developer team simulation. The graph will be used to model the interactions between the analyst, coder, tester, and reviewer.\n",
      "        \n",
      "\n",
      "            - \"LangGraph documentation\"\n",
      "            - \"graph-based language models explained\"\n",
      "            - \"nodes and edges in LangGraph\"\n",
      "            - \"how to create a graph using LangGraph in Python\"\n",
      "        \n",
      "\n",
      "            Implement the pydantic-ai library to handle data validation and parsing within the developer team simulation. This step involves creating schemas for each role (analyst, coder, tester, reviewer) and ensuring that all inputs and outputs conform to these schemas. The goal is to maintain data consistency and integrity throughout the simulation.\n",
      "        \n",
      "\n",
      "            - \"pydantic-ai tutorial\"\n",
      "            - \"data validation with pydantic\"\n",
      "            - \"how to create schemas in pydantic\"\n",
      "            - \"parsing data with pydantic-ai\"\n",
      "        \n",
      "\n",
      "            Integrate the Ollama model into the project. This involves setting up the Ollama server, configuring it to work with the LangGraph framework, and ensuring that the model can be accessed and used by each component of the developer team simulation. The integration should allow for seamless communication between the different roles.\n",
      "        \n",
      "\n",
      "            - \"Ollama model installation\"\n",
      "            - \"how to integrate Ollama with Python\"\n",
      "            - \"Ollama server setup guide\"\n",
      "            - \"communication between LangGraph and Ollama\"\n",
      "        \n",
      "\n",
      "            Design and implement the developer team simulation. This involves creating classes or modules for each role (analyst, coder, tester, reviewer) and defining their responsibilities and interactions within the system. Each role should be able to communicate with the others through the LangGraph framework and use the Ollama model to perform their tasks.\n",
      "        \n",
      "\n",
      "            - \"object-oriented design in Python\"\n",
      "            - \"class modules for developer roles\"\n",
      "            - \"interactions between classes in a simulation\"\n",
      "            - \"how to implement team roles in Python\"\n",
      "        \n",
      "\n",
      "            Test the integration of all components. This involves running simulations where each role performs their tasks, and ensuring that the interactions are smooth and produce the expected results. Testing should cover various scenarios to ensure robustness and reliability.\n",
      "        \n",
      "\n",
      "            - \"unit testing in Python\"\n",
      "            - \"integration testing for complex systems\"\n",
      "            - \"how to test simulations in Python\"\n",
      "            - \"debugging tips for simulation code\"\n",
      "        \n",
      "\n",
      "            Refine and optimize the system based on testing results. This involves identifying any bottlenecks or issues that arose during testing and implementing fixes or improvements. The goal is to create a seamless and efficient developer team simulation that accurately models real-world interactions.\n",
      "        \n",
      "\n",
      "            - \"optimization techniques in Python\"\n",
      "            - \"how to improve simulation performance\"\n",
      "            - \"debugging complex systems\"\n",
      "            - \"best practices for code optimization\"\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "for s in steps:\n",
    "    print(s.description)\n",
    "    print(s.web_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "14 validation errors for Steps\nsteps.0.title\n  Field required [type=missing, input_value={'step_description': \"Cla...st'.\", 'step_number': 1}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.0.description\n  Field required [type=missing, input_value={'step_description': \"Cla...st'.\", 'step_number': 1}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.1.title\n  Field required [type=missing, input_value={'step_description': \"Ide...hem.\", 'step_number': 2}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.1.description\n  Field required [type=missing, input_value={'step_description': \"Ide...hem.\", 'step_number': 2}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.2.title\n  Field required [type=missing, input_value={'step_description': \"Def...ess.\", 'step_number': 3}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.2.description\n  Field required [type=missing, input_value={'step_description': \"Def...ess.\", 'step_number': 3}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.3.title\n  Field required [type=missing, input_value={'step_description': 'Tes...nce.', 'step_number': 4}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.3.description\n  Field required [type=missing, input_value={'step_description': 'Tes...nce.', 'step_number': 4}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.4.title\n  Field required [type=missing, input_value={'step_description': 'Tes...oth.', 'step_number': 5}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.4.description\n  Field required [type=missing, input_value={'step_description': 'Tes...oth.', 'step_number': 5}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.5.title\n  Field required [type=missing, input_value={'step_description': 'Res...ngs.', 'step_number': 6}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.5.description\n  Field required [type=missing, input_value={'step_description': 'Res...ngs.', 'step_number': 6}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.6.title\n  Field required [type=missing, input_value={'step_description': 'Rep...ble.', 'step_number': 7}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.6.description\n  Field required [type=missing, input_value={'step_description': 'Rep...ble.', 'step_number': 7}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyst_llm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mThis is a test\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3029\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3027\u001b[39m             \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m   3028\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3029\u001b[39m             \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3030\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3031\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py:193\u001b[39m, in \u001b[36mBaseOutputParser.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    187\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    188\u001b[39m     \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage],\n\u001b[32m    189\u001b[39m     config: Optional[RunnableConfig] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    190\u001b[39m     **kwargs: Any,\n\u001b[32m    191\u001b[39m ) -> T:\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    202\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    203\u001b[39m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=inner_input)]),\n\u001b[32m    204\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    205\u001b[39m             config,\n\u001b[32m    206\u001b[39m             run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    207\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:1927\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1923\u001b[39m     context = copy_context()\n\u001b[32m   1924\u001b[39m     context.run(_set_config_context, child_config)\n\u001b[32m   1925\u001b[39m     output = cast(\n\u001b[32m   1926\u001b[39m         Output,\n\u001b[32m-> \u001b[39m\u001b[32m1927\u001b[39m         \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1928\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1929\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1930\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1931\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1932\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1933\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1934\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1935\u001b[39m     )\n\u001b[32m   1936\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1937\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:396\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    395\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py:194\u001b[39m, in \u001b[36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[39m\u001b[34m(inner_input)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    187\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    188\u001b[39m     \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage],\n\u001b[32m    189\u001b[39m     config: Optional[RunnableConfig] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    190\u001b[39m     **kwargs: Any,\n\u001b[32m    191\u001b[39m ) -> T:\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m    193\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    197\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    198\u001b[39m             config,\n\u001b[32m    199\u001b[39m             run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    200\u001b[39m         )\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    202\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    203\u001b[39m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=inner_input)]),\n\u001b[32m    204\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    205\u001b[39m             config,\n\u001b[32m    206\u001b[39m             run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    207\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/langchain_core/output_parsers/openai_tools.py:294\u001b[39m, in \u001b[36mPydanticToolsParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     pydantic_objects.append(\u001b[43mname_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43margs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ValidationError, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m    296\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m partial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/pydantic/main.py:214\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    213\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    216\u001b[39m     warnings.warn(\n\u001b[32m    217\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    218\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    219\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    220\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    221\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 14 validation errors for Steps\nsteps.0.title\n  Field required [type=missing, input_value={'step_description': \"Cla...st'.\", 'step_number': 1}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.0.description\n  Field required [type=missing, input_value={'step_description': \"Cla...st'.\", 'step_number': 1}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.1.title\n  Field required [type=missing, input_value={'step_description': \"Ide...hem.\", 'step_number': 2}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.1.description\n  Field required [type=missing, input_value={'step_description': \"Ide...hem.\", 'step_number': 2}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.2.title\n  Field required [type=missing, input_value={'step_description': \"Def...ess.\", 'step_number': 3}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.2.description\n  Field required [type=missing, input_value={'step_description': \"Def...ess.\", 'step_number': 3}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.3.title\n  Field required [type=missing, input_value={'step_description': 'Tes...nce.', 'step_number': 4}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.3.description\n  Field required [type=missing, input_value={'step_description': 'Tes...nce.', 'step_number': 4}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.4.title\n  Field required [type=missing, input_value={'step_description': 'Tes...oth.', 'step_number': 5}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.4.description\n  Field required [type=missing, input_value={'step_description': 'Tes...oth.', 'step_number': 5}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.5.title\n  Field required [type=missing, input_value={'step_description': 'Res...ngs.', 'step_number': 6}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.5.description\n  Field required [type=missing, input_value={'step_description': 'Res...ngs.', 'step_number': 6}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.6.title\n  Field required [type=missing, input_value={'step_description': 'Rep...ble.', 'step_number': 7}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsteps.6.description\n  Field required [type=missing, input_value={'step_description': 'Rep...ble.', 'step_number': 7}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing"
     ]
    }
   ],
   "source": [
    "agent.analyst_llm.invoke({\"question\": \"This is a test\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "llm = ChatOllama(model=\"deepseek-r1:14b\", temperature=0.0, base_url=\"http://localhost:8888\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\n\\n</think>\\n\\nIt seems like you're testing or checking something. How can I assist you with this? If you have any questions or need information, feel free to ask!\", additional_kwargs={}, response_metadata={'model': 'deepseek-r1:14b', 'created_at': '2025-03-22T07:34:15.095737642Z', 'done': True, 'done_reason': 'stop', 'total_duration': 562101498, 'load_duration': 67535935, 'prompt_eval_count': 7, 'prompt_eval_duration': 4667842, 'eval_count': 37, 'eval_duration': 489594147, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-b9e36163-25e7-46b8-a7d3-ab8a706bb72b-0', usage_metadata={'input_tokens': 7, 'output_tokens': 37, 'total_tokens': 44})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"This is a test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"You are the best to split a coding request into several small steps easily implementable by a junior developer.\n",
    "\n",
    "- You read the question and split it into several steps to be able to analyse it in depth and provide the more precise answer you can.\n",
    "- Your answer will not contain any code.\n",
    "- Your final steps will be used by your software engineers team to implement a version of the request.\n",
    "- Your final steps must be as developed as possible so that developers have as little research as possible to do.\n",
    "- For each step, add a title, a description and a web search that can help to implement it.\n",
    "- The programming language is Python.\n",
    "\n",
    "Your response has to follow meticulously the following structure:\n",
    "\n",
    "- step 1:\n",
    "  This step consists in ...\n",
    "- step 2:\n",
    "  This step consists in ...\n",
    "...\n",
    "- step n:\n",
    "  This step consists in ...\n",
    "\n",
    "Each step will be writing in markdown format.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", PROMPT),\n",
    "        (\"human\", \"The Python code to implement is related to this task: {task}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_code = llm.with_structured_output(Steps)\n",
    "junior_llm = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = junior_llm.invoke(\"Create a function that reads a png file and return a NumPy array with YUV format using PIL package and nothing else.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, I need to create a Python function that reads a PNG file and returns a NumPy array in YUV format using only the PIL package. Let me break down how I can approach this.\n",
       "\n",
       "First, I know that PIL's Image module can open PNG files, so step one is straightforwardâ€”using Image.open to read the image. But since the output needs to be a NumPy array, I'll have to convert the PIL Image object into a NumPy array.\n",
       "\n",
       "Next, I remember that PIL images are in RGB format by default. So after converting to a NumPy array, I need to transform this RGB array into YUV. I'm not exactly sure about the exact formulas for YUV conversion from RGB, but I recall there are standard ones. Maybe I should look up the YCbCr color space since that's related to YUV.\n",
       "\n",
       "I think the steps after getting the NumPy array would involve extracting the R, G, B channels and then applying the mathematical transformations to get Y, U, V (or maybe Y, Cb, Cr). I'll need to set up new arrays for each of these components. \n",
       "\n",
       "Wait, do I need to normalize the pixel values? Since PIL images are typically in 0-255 range, but when converted to NumPy, they might be integers. For accurate calculations, perhaps converting them to floats would be better.\n",
       "\n",
       "I should also consider whether the image is in the correct mode before conversion. Maybe ensuring it's in RGB mode to avoid any issues during the array conversion.\n",
       "\n",
       "Putting it all together, I'll outline each step clearly so that a junior developer can follow without much hassle. Each step will have a title, description, and a helpful web search to guide them through the process.\n",
       "</think>\n",
       "\n",
       "Let me break down this task into several small steps:\n",
       "\n",
       "- step 1: Read the PNG file using PIL\n",
       "  - This step consists in reading an image file using Python Imaging Library (PIL) package.\n",
       "  - Description: Use Image.open() method from PIL to read the PNG file. Make sure the file exists and is accessible.\n",
       "  - Web search: \"Python PIL open png file\"\n",
       "\n",
       "- step 2: Convert the PIL Image object to a NumPy array\n",
       "  - This step consists in converting the PIL Image object into a NumPy array for further processing.\n",
       "  - Description: Use numpy.array() function to convert the PIL Image object into a NumPy array. Ensure the data type is appropriate (uint8).\n",
       "  - Web search: \"convert pil image to numpy array\"\n",
       "\n",
       "- step 3: Convert RGB colorspace to YUV\n",
       "  - This step consists in converting the RGB color space of the NumPy array to YUV.\n",
       "  - Description: Apply mathematical transformations to convert each pixel's RGB values to YUV. Use standard YCbCr conversion formulas.\n",
       "  - Web search: \"rgb to yuv conversion formula\"\n",
       "\n",
       "- step 4: Return the resulting NumPy array\n",
       "  - This step consists in returning the final NumPy array with YUV data from the function.\n",
       "  - Description: Ensure the array is properly formatted and normalized before returning it.\n",
       "\n",
       "Each of these steps can be implemented sequentially, ensuring proper error handling and data type management."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.openai import OpenAIModel, OpenAIModelSettings\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "from pydantic_ai.common_tools.duckduckgo import duckduckgo_search_tool\n",
    "\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\")\n",
    "SPLIT_MODEL = os.getenv(\"SPLIT_MODEL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ollama_model = OpenAIModel(\n",
    "    model_name=\"llama3.3:latest\",\n",
    "    provider=OpenAIProvider(base_url=OLLAMA_BASE_URL + \"/v1/\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class Step(BaseModel):\n",
    "    \"\"\"Describe one step found in the user message.\"\"\"\n",
    "\n",
    "    description: str = Field(description=\"The description of the step\")\n",
    "    web_search: str = Field(description=\"A web search that can help to implement the step\")\n",
    "\n",
    "\n",
    "class Steps(BaseModel):\n",
    "    \"\"\"The steps defined in the user message split in several parts\"\"\"\n",
    "\n",
    "    # context: str = Field(description=\"The introduction of the message (all text before the steps description)\")\n",
    "    steps: List[Step] = Field(\n",
    "        description=\"Each entry is composed by a step mentioned in the message\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "split_agent = Agent(\n",
    "    model=ollama_model,\n",
    "    system_prompt=\"Split the steps of the user message using the result type. You change nothing about the content of this message, just split it.\",\n",
    "    model_settings=OpenAIModelSettings(\n",
    "        temperature=0.0,\n",
    "    ),\n",
    "    retries=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnexpectedModelBehavior",
     "evalue": "Exceeded maximum retries (4) for result validation",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/pydantic_ai/_result.py:205\u001b[39m, in \u001b[36mResultTool.validate\u001b[39m\u001b[34m(self, tool_call, allow_partial, wrap_validation_errors)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool_call.args, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtype_adapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_call\u001b[49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperimental_allow_partial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpyd_allow_partial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/pydantic/type_adapter.py:446\u001b[39m, in \u001b[36mTypeAdapter.validate_json\u001b[39m\u001b[34m(self, data, strict, context, experimental_allow_partial)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Usage docs: https://docs.pydantic.dev/2.10/concepts/json/#json-parsing\u001b[39;00m\n\u001b[32m    430\u001b[39m \n\u001b[32m    431\u001b[39m \u001b[33;03mValidate a JSON string or bytes against the model.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    444\u001b[39m \u001b[33;03m    The validated object.\u001b[39;00m\n\u001b[32m    445\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_partial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperimental_allow_partial\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 8 validation errors for Steps\nsteps.0\n  Input should be an object [type=model_type, input_value='Set up the development environment', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\nsteps.1\n  Input should be an object [type=model_type, input_value='Initialize a new LangGraph project', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\nsteps.2\n  Input should be an object [type=model_type, input_value='Define team roles using Pydantic models', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\nsteps.3\n  Input should be an object [type=model_type, input_value='Create a workflow using LangGraph', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\nsteps.4\n  Input should be an object [type=model_type, input_value='Integrate Ollama models into the workflow', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\nsteps.5\n  Input should be an object [type=model_type, input_value='Implement the team roles as LangGraph nodes', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\nsteps.6\n  Input should be an object [type=model_type, input_value='Test the workflow', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type\nsteps.7\n  Input should be an object [type=model_type, input_value='Document the project', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mToolRetryError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:433\u001b[39m, in \u001b[36mCallToolsNode._handle_tool_calls\u001b[39m\u001b[34m(self, ctx, tool_calls)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     result_data = \u001b[43mresult_tool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m     result_data = \u001b[38;5;28;01mawait\u001b[39;00m _validate_result(result_data, ctx, call)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/pydantic_ai/_result.py:215\u001b[39m, in \u001b[36mResultTool.validate\u001b[39m\u001b[34m(self, tool_call, allow_partial, wrap_validation_errors)\u001b[39m\n\u001b[32m    210\u001b[39m     m = _messages.RetryPromptPart(\n\u001b[32m    211\u001b[39m         tool_name=tool_call.tool_name,\n\u001b[32m    212\u001b[39m         content=e.errors(include_url=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m    213\u001b[39m         tool_call_id=tool_call.tool_call_id,\n\u001b[32m    214\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ToolRetryError(m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mToolRetryError\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mUnexpectedModelBehavior\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m split_text = \u001b[43msplit_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSteps\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py:558\u001b[39m, in \u001b[36mAgent.run_sync\u001b[39m\u001b[34m(self, user_prompt, result_type, message_history, model, deps, model_settings, usage_limits, usage, infer_name)\u001b[39m\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m infer_name \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    557\u001b[39m     \u001b[38;5;28mself\u001b[39m._infer_name(inspect.currentframe())\n\u001b[32m--> \u001b[39m\u001b[32m558\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_event_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresult_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresult_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage_history\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m        \u001b[49m\u001b[43musage_limits\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_limits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[43musage\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m        \u001b[49m\u001b[43minfer_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/futures.py:202\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py:316\u001b[39m, in \u001b[36mAgent.run\u001b[39m\u001b[34m(self, user_prompt, result_type, message_history, model, deps, model_settings, usage_limits, usage, infer_name)\u001b[39m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28mself\u001b[39m._infer_name(inspect.currentframe())\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(\n\u001b[32m    307\u001b[39m     user_prompt=user_prompt,\n\u001b[32m    308\u001b[39m     result_type=result_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m    314\u001b[39m     usage=usage,\n\u001b[32m    315\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m agent_run:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m agent_run:\n\u001b[32m    317\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (final_result := agent_run.result) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m'\u001b[39m\u001b[33mThe graph run did not finish properly\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py:1366\u001b[39m, in \u001b[36mAgentRun.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__anext__\u001b[39m(\n\u001b[32m   1363\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1364\u001b[39m ) -> _agent_graph.AgentNode[AgentDepsT, ResultDataT] | End[FinalResult[ResultDataT]]:\n\u001b[32m   1365\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Advance to the next node automatically based on the last returned node.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1366\u001b[39m     next_node = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._graph_run.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1367\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _agent_graph.is_agent_node(next_node):\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m next_node\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py:782\u001b[39m, in \u001b[36mGraphRun.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._next_node, End):\n\u001b[32m    781\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.next(\u001b[38;5;28mself\u001b[39m._next_node)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py:760\u001b[39m, in \u001b[36mGraphRun.next\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.persistence.record_run(node_snapshot_id):\n\u001b[32m    759\u001b[39m         ctx = GraphRunContext(\u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.deps)\n\u001b[32m--> \u001b[39m\u001b[32m760\u001b[39m         \u001b[38;5;28mself\u001b[39m._next_node = \u001b[38;5;28;01mawait\u001b[39;00m node.run(ctx)\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._next_node, End):\n\u001b[32m    763\u001b[39m     \u001b[38;5;28mself\u001b[39m._snapshot_id = \u001b[38;5;28mself\u001b[39m._next_node.get_snapshot_id()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:365\u001b[39m, in \u001b[36mCallToolsNode.run\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\n\u001b[32m    363\u001b[39m     \u001b[38;5;28mself\u001b[39m, ctx: GraphRunContext[GraphAgentState, GraphAgentDeps[DepsT, NodeRunEndT]]\n\u001b[32m    364\u001b[39m ) -> Union[ModelRequestNode[DepsT, NodeRunEndT], End[result.FinalResult[NodeRunEndT]]]:  \u001b[38;5;66;03m# noqa UP007\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream(ctx):\n\u001b[32m    366\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    368\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m (next_node := \u001b[38;5;28mself\u001b[39m._next_node) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m'\u001b[39m\u001b[33mthe stream should set `self._next_node` before it ends\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:217\u001b[39m, in \u001b[36m_AsyncGeneratorContextManager.__aexit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m anext(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m    219\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:380\u001b[39m, in \u001b[36mCallToolsNode.stream\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m stream\n\u001b[32m    379\u001b[39m \u001b[38;5;66;03m# Run the stream to completion if it was not finished:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _event \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:417\u001b[39m, in \u001b[36mCallToolsNode._run_stream\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    413\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m exceptions.UnexpectedModelBehavior(\u001b[33m'\u001b[39m\u001b[33mReceived empty model response\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    415\u001b[39m     \u001b[38;5;28mself\u001b[39m._events_iterator = _run_stream()\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._events_iterator:\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:407\u001b[39m, in \u001b[36mCallToolsNode._run_stream.<locals>._run_stream\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;66;03m# At the moment, we prioritize at least executing tool calls if they are present.\u001b[39;00m\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# In the future, we'd consider making this configurable at the agent or run level.\u001b[39;00m\n\u001b[32m    404\u001b[39m \u001b[38;5;66;03m# This accounts for cases like anthropic returns that might contain a text response\u001b[39;00m\n\u001b[32m    405\u001b[39m \u001b[38;5;66;03m# and a tool call response, where the text response just indicates the tool call will happen.\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tool_calls:\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handle_tool_calls(ctx, tool_calls):\n\u001b[32m    408\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m texts:\n\u001b[32m    410\u001b[39m     \u001b[38;5;66;03m# No events are emitted during the handling of text responses, so we don't need to yield anything\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:438\u001b[39m, in \u001b[36mCallToolsNode._handle_tool_calls\u001b[39m\u001b[34m(self, ctx, tool_calls)\u001b[39m\n\u001b[32m    434\u001b[39m     result_data = \u001b[38;5;28;01mawait\u001b[39;00m _validate_result(result_data, ctx, call)\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _result.ToolRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    436\u001b[39m     \u001b[38;5;66;03m# TODO: Should only increment retry stuff once per node execution, not for each tool call\u001b[39;00m\n\u001b[32m    437\u001b[39m     \u001b[38;5;66;03m#   Also, should increment the tool-specific retry count rather than the run retry count\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m     \u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement_retries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeps\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_result_retries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m     parts.append(e.tool_retry)\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/LLM/code_agent/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:72\u001b[39m, in \u001b[36mGraphAgentState.increment_retries\u001b[39m\u001b[34m(self, max_result_retries)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28mself\u001b[39m.retries += \u001b[32m1\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.retries > max_result_retries:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.UnexpectedModelBehavior(\n\u001b[32m     73\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mExceeded maximum retries (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_result_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) for result validation\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     74\u001b[39m     )\n",
      "\u001b[31mUnexpectedModelBehavior\u001b[39m: Exceeded maximum retries (4) for result validation"
     ]
    }
   ],
   "source": [
    "split_text = split_agent.run_sync(response.content, result_type=Steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Steps(steps=['Set up the development environment', 'Initialize a new LangGraph project', 'Define team roles using Pydantic models', 'Create a workflow using LangGraph', 'Integrate Ollama models into the workflow', 'Implement the team roles as LangGraph nodes', 'Test the workflow', 'Document the project'])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_text.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, I need to figure out how to split the user\\'s request into manageable steps for a junior developer. The task is to write a project using LangGraph and pydantic-ai with Ollama models, simulating a developer team with roles: analyst, coder, tester, and reviewer.\\n\\nFirst, I should understand each component involved. LangGraph is a framework for building language models, and pydantic-ai adds validation using Pydantic. Ollama is another model, so we\\'ll need to integrate that as well.\\n\\nI think the first step is setting up the environment because without the right tools installed, nothing else can be done. So, installing Python, virtual environments, and the necessary packages like langgraph, pydantic-ai, and ollama makes sense here.\\n\\nNext, initializing a LangGraph project will provide the basic structure needed for the application. This step is crucial as it sets up the foundation upon which everything else will be built.\\n\\nThen, defining the team roles using Pydantic models seems logical. Each role (analyst, coder, etc.) can have its own model with specific fields and methods. Using Pydantic ensures data validation, which is important for maintaining data integrity.\\n\\nCreating a workflow that orchestrates these roles is the next step. LangGraph\\'s graph structure will help in defining how each role interacts and passes tasks along. This step ties everything together by creating a process flow.\\n\\nIntegrating Ollama models into the workflow means specifying how each team member uses Ollama for their tasks. This could involve setting up API calls or model loading within each role\\'s function.\\n\\nImplementing each team member as a node in LangGraph will make the workflow functional. Each node corresponds to a role and has its own logic, which is essential for simulating the team\\'s behavior accurately.\\n\\nTesting the implementation ensures that all parts work together without issues. This step is vital for catching any bugs or misconfigurations early on.\\n\\nFinally, documenting the project helps in maintaining it and makes it easier for others to understand and contribute. Good documentation covers how each part works and how they interact.\\n\\nI should make sure each step is clear and provides enough detail so that a junior developer can follow without too much research. Including web searches will guide them to resources if they need more information on any step.\\n</think>\\n\\nLet me break down the task into smaller, manageable steps:\\n\\n- step 1: Set up the development environment\\n  - This step consists in setting up Python and installing required packages (langgraph, pydantic-ai, ollama).\\n  - Description: Create a virtual environment, install necessary libraries.\\n  - Web search: \"Python virtual environment setup\"\\n\\n- step 2: Initialize a new LangGraph project\\n  - This step consists in creating a new project using LangGraph and setting up the basic structure.\\n  - Description: Use langgraph CLI to initialize a new project.\\n  - Web search: \"langgraph initialize project\"\\n\\n- step 3: Define team roles using Pydantic models\\n  - This step consists in creating Pydantic models for each team role (analyst, coder, tester, reviewer).\\n  - Description: Define data structures for each role\\'s responsibilities and outputs.\\n  - Web search: \"pydantic model definition\"\\n\\n- step 4: Create a workflow using LangGraph\\n  - This step consists in defining the workflow graph that connects the different team roles.\\n  - Description: Design the process flow where tasks are passed between roles.\\n  - Web search: \"langgraph workflow design\"\\n\\n- step 5: Integrate Ollama models into the workflow\\n  - This step consists in setting up Ollama as the AI model provider for each team role.\\n  - Description: Configure Ollama API calls within the LangGraph nodes.\\n  - Web search: \"ollama integration with Python\"\\n\\n- step 6: Implement the team roles as LangGraph nodes\\n  - This step consists in writing the code that implements each team role as a separate node in the graph.\\n  - Description: Create functions for analysis, coding, testing, and reviewing tasks.\\n  - Web search: \"langgraph custom nodes implementation\"\\n\\n- step 7: Test the workflow\\n  - This step consists in running test cases to ensure the workflow works as expected.\\n  - Description: Validate each role\\'s functionality and data flow between them.\\n  - Web search: \"testing LangGraph workflows\"\\n\\n- step 8: Document the project\\n  - This step consists in writing documentation for the project, including setup instructions and usage guide.\\n  - Description: Create a README file explaining how to run the simulation.\\n  - Web search: \"Python project documentation best practices\"'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
